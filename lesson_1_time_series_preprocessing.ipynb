{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1: Time Series Data & Preprocessing\n",
    "\n",
    "## Learning Objectives\n",
    "- LO6: Understand what time series data is and how it differs from sequential data\n",
    "- LO7: Apply techniques for transforming time series data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Opening Activity - \"What Do You See?\"\n",
    "\n",
    "Let's start by generating and visualizing three different types of data patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate sample data for opening activity\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Daily temperature over a year\n",
    "days = np.arange(365)\n",
    "temperature = 15 + 10 * np.sin(2 * np.pi * days / 365) + np.random.normal(0, 2, 365)\n",
    "\n",
    "# 2. Text as sequence (letter positions in alphabet)\n",
    "text = \"TIMESERIES\"\n",
    "letter_values = [ord(char) - ord('A') + 1 for char in text]\n",
    "\n",
    "# 3. Sensor readings over time (with trend and noise)\n",
    "time_points = np.arange(100)\n",
    "sensor_readings = 50 + 0.3 * time_points + 5 * np.sin(time_points / 5) + np.random.normal(0, 3, 100)\n",
    "\n",
    "# Visualize all three\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(days, temperature, linewidth=1, color='orange')\n",
    "axes[0].set_title('Pattern A', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Index')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(range(len(letter_values)), letter_values, marker='o', linewidth=2, markersize=8, color='blue')\n",
    "axes[1].set_title('Pattern B', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Index')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(time_points, sensor_readings, linewidth=1, color='green')\n",
    "axes[2].set_title('Pattern C', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Index')\n",
    "axes[2].set_ylabel('Value')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nü§î Discussion Questions:\")\n",
    "print(\"- Which two patterns look similar to each other?\")\n",
    "print(\"- What makes them similar?\")\n",
    "print(\"- What role does TIME play in each pattern?\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí≠ Reflection (Discuss with your neighbor)\n",
    "\n",
    "Write your observations here:\n",
    "- Patterns A and C are: ___________\n",
    "- Pattern B is different because: ___________\n",
    "- Time is important for: ___________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Time Series vs Sequential Data\n",
    "\n",
    "### Key Definitions\n",
    "\n",
    "**Time Series Data:**\n",
    "- Observations ordered in time with **regular intervals**\n",
    "- Time itself is intrinsically important\n",
    "- Examples: stock prices, temperature readings, heart rate per second\n",
    "\n",
    "**Sequential Data:**\n",
    "- Data where **order matters** but time intervals may not be regular or important\n",
    "- Examples: DNA sequences, text, browsing history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 1: Categorization\n",
    "\n",
    "Classify each example as either **Time Series (TS)** or **Sequential (SEQ)**:\n",
    "\n",
    "1. DNA sequence: _______\n",
    "2. Heart rate per second: _______\n",
    "3. Search history: _______\n",
    "4. Daily sales figures: _______\n",
    "5. Words in a sentence: _______\n",
    "6. Hourly energy consumption: _______\n",
    "7. Customer purchases on a website: _______\n",
    "8. Temperature readings every 10 minutes: _______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üë• Group Activity (15 minutes)\n",
    "\n",
    "**Your domain:** ___________  (healthcare / retail / industry / transport)\n",
    "\n",
    "**Brainstorm:**\n",
    "- 2 time series examples from your domain:\n",
    "  1. ___________\n",
    "  2. ___________\n",
    "\n",
    "- 2 sequential data examples from your domain:\n",
    "  1. ___________\n",
    "  2. ___________\n",
    "\n",
    "**Discussion:** Why is this distinction important for analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Characteristics of Time Series\n",
    "\n",
    "Time series data typically contains three main components:\n",
    "1. **Trend**: Long-term increase or decrease\n",
    "2. **Seasonality**: Regular, repeating patterns\n",
    "3. **Noise**: Random, irregular fluctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Sample Energy Consumption Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create synthetic energy consumption data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Time range: 2 years of hourly data\n",
    "hours = pd.date_range('2022-01-01', periods=24*365*2, freq='H')\n",
    "n = len(hours)\n",
    "\n",
    "# Components\n",
    "trend = np.linspace(100, 120, n)  # Gradual increase in consumption\n",
    "seasonal_yearly = 15 * np.sin(2 * np.pi * np.arange(n) / (24*365))  # Yearly seasonality\n",
    "seasonal_daily = 10 * np.sin(2 * np.pi * np.arange(n) / 24)  # Daily seasonality\n",
    "noise = np.random.normal(0, 3, n)  # Random noise\n",
    "\n",
    "# Combine all components\n",
    "energy_consumption = trend + seasonal_yearly + seasonal_daily + noise\n",
    "\n",
    "# Create DataFrame\n",
    "df_energy = pd.DataFrame({\n",
    "    'timestamp': hours,\n",
    "    'consumption': energy_consumption,\n",
    "    'trend': trend,\n",
    "    'seasonal_yearly': seasonal_yearly,\n",
    "    'seasonal_daily': seasonal_daily,\n",
    "    'noise': noise\n",
    "})\n",
    "\n",
    "print(f\"Dataset created: {len(df_energy)} hours of data\")\n",
    "print(f\"Date range: {df_energy['timestamp'].min()} to {df_energy['timestamp'].max()}\")\n",
    "df_energy.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Exercise 2: Visualize the Data\n",
    "\n",
    "**Task:** Plot the energy consumption data and identify its components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the complete time series\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(df_energy['timestamp'], df_energy['consumption'], linewidth=0.5, alpha=0.8)\n",
    "plt.title('Energy Consumption Over Time', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Energy Consumption (kWh)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Your Analysis:\n",
    "\n",
    "Look at the plot above and answer:\n",
    "\n",
    "1. **Trend**: Do you see an overall increase, decrease, or stability? ___________\n",
    "\n",
    "2. **Seasonality**: Do you notice any repeating patterns? At what frequency? ___________\n",
    "\n",
    "3. **Noise**: How much random variation is present? ___________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decompose the Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize individual components\n",
    "fig, axes = plt.subplots(4, 1, figsize=(15, 10))\n",
    "\n",
    "# Plot first 30 days for clarity\n",
    "sample_days = 30\n",
    "sample_data = df_energy.iloc[:24*sample_days]\n",
    "\n",
    "axes[0].plot(sample_data['timestamp'], sample_data['consumption'], linewidth=1)\n",
    "axes[0].set_title('Original Series', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(sample_data['timestamp'], sample_data['trend'], color='red', linewidth=2)\n",
    "axes[1].set_title('Trend Component', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(sample_data['timestamp'], sample_data['seasonal_yearly'] + sample_data['seasonal_daily'], color='green', linewidth=1)\n",
    "axes[2].set_title('Seasonal Component', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('Value')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "axes[3].plot(sample_data['timestamp'], sample_data['noise'], color='gray', linewidth=0.5, alpha=0.7)\n",
    "axes[3].set_title('Noise Component', fontsize=12, fontweight='bold')\n",
    "axes[3].set_xlabel('Time')\n",
    "axes[3].set_ylabel('Value')\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí¨ Think-Pair-Share\n",
    "\n",
    "**Individual:** Which component do you see most strongly in the original data?\n",
    "\n",
    "Your answer: ___________\n",
    "\n",
    "**Pair:** Compare with your neighbor\n",
    "\n",
    "**Share:** Be ready to share your findings with the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö Key Concept: Stationarity\n",
    "\n",
    "**Stationary Time Series:**\n",
    "- Statistical properties (mean, variance) remain constant over time\n",
    "- No trend, no seasonality\n",
    "- Important for many forecasting models\n",
    "\n",
    "**Non-Stationary Time Series:**\n",
    "- Statistical properties change over time\n",
    "- Has trend and/or seasonality\n",
    "- Often needs transformation before modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize stationary vs non-stationary\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 4))\n",
    "\n",
    "# Stationary series (white noise)\n",
    "stationary = np.random.normal(50, 5, 500)\n",
    "axes[0].plot(stationary, linewidth=1)\n",
    "axes[0].axhline(y=np.mean(stationary), color='red', linestyle='--', label=f'Mean = {np.mean(stationary):.1f}')\n",
    "axes[0].set_title('Stationary Series', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Time')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Non-stationary series (with trend)\n",
    "non_stationary = 30 + 0.1 * np.arange(500) + np.random.normal(0, 5, 500)\n",
    "axes[1].plot(non_stationary, linewidth=1)\n",
    "axes[1].plot(30 + 0.1 * np.arange(500), color='red', linestyle='--', linewidth=2, label='Trend')\n",
    "axes[1].set_title('Non-Stationary Series', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ùì Why is stationarity important?\")\n",
    "print(\"Many statistical models assume constant statistical properties.\")\n",
    "print(\"Non-stationary data often needs transformation first!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚òï BREAK (10 minutes)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Data Transformation Techniques\n",
    "\n",
    "Real-world time series data often has problems:\n",
    "- Missing values\n",
    "- Outliers\n",
    "- Too much noise\n",
    "- Different scales\n",
    "\n",
    "Let's learn how to fix these!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create \"Dirty\" Data for Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a problematic time series\n",
    "np.random.seed(42)\n",
    "n_points = 200\n",
    "time = pd.date_range('2023-01-01', periods=n_points, freq='H')\n",
    "\n",
    "# Base signal\n",
    "clean_signal = 50 + 10 * np.sin(2 * np.pi * np.arange(n_points) / 24) + 0.1 * np.arange(n_points)\n",
    "\n",
    "# Add problems\n",
    "dirty_signal = clean_signal.copy()\n",
    "dirty_signal += np.random.normal(0, 5, n_points)  # Add noise\n",
    "\n",
    "# Add missing values (10% of data)\n",
    "missing_indices = np.random.choice(n_points, size=int(n_points * 0.1), replace=False)\n",
    "dirty_signal[missing_indices] = np.nan\n",
    "\n",
    "# Add outliers (5% of data)\n",
    "outlier_indices = np.random.choice(n_points, size=int(n_points * 0.05), replace=False)\n",
    "dirty_signal[outlier_indices] += np.random.choice([-1, 1], size=len(outlier_indices)) * np.random.uniform(30, 50, size=len(outlier_indices))\n",
    "\n",
    "# Create DataFrame\n",
    "df_dirty = pd.DataFrame({\n",
    "    'timestamp': time,\n",
    "    'value': dirty_signal,\n",
    "    'clean_value': clean_signal\n",
    "})\n",
    "\n",
    "# Visualize the problem\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(df_dirty['timestamp'], df_dirty['value'], 'o-', markersize=3, linewidth=0.5, label='Dirty Data', alpha=0.7)\n",
    "plt.plot(df_dirty['timestamp'], df_dirty['clean_value'], linewidth=2, label='Clean Signal', alpha=0.8)\n",
    "plt.title('\"Dirty\" Time Series Data', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Data Quality Report:\")\n",
    "print(f\"Total points: {len(df_dirty)}\")\n",
    "print(f\"Missing values: {df_dirty['value'].isna().sum()} ({df_dirty['value'].isna().sum()/len(df_dirty)*100:.1f}%)\")\n",
    "print(f\"Potential outliers detected: ~{int(n_points * 0.05)}\")\n",
    "print(f\"\\nüéØ Goal: Clean this data to make it analyzable!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Technique 1: Dealing with Missing Data (6 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Different strategies for handling missing values\n",
    "df_missing = df_dirty.copy()\n",
    "\n",
    "# Method 1: Forward fill\n",
    "ffill_values = df_missing['value'].fillna(method='ffill')\n",
    "\n",
    "# Method 2: Backward fill\n",
    "bfill_values = df_missing['value'].fillna(method='bfill')\n",
    "\n",
    "# Method 3: Linear interpolation\n",
    "interp_values = df_missing['value'].interpolate(method='linear')\n",
    "\n",
    "# Method 4: Mean imputation\n",
    "mean_values = df_missing['value'].fillna(df_missing['value'].mean())\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "axes[0, 0].plot(df_missing['timestamp'], ffill_values, linewidth=1)\n",
    "axes[0, 0].scatter(df_missing['timestamp'][missing_indices], ffill_values[missing_indices], color='red', s=50, zorder=5, label='Imputed')\n",
    "axes[0, 0].set_title('Forward Fill', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(df_missing['timestamp'], bfill_values, linewidth=1)\n",
    "axes[0, 1].scatter(df_missing['timestamp'][missing_indices], bfill_values[missing_indices], color='red', s=50, zorder=5, label='Imputed')\n",
    "axes[0, 1].set_title('Backward Fill', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(df_missing['timestamp'], interp_values, linewidth=1)\n",
    "axes[1, 0].scatter(df_missing['timestamp'][missing_indices], interp_values[missing_indices], color='red', s=50, zorder=5, label='Imputed')\n",
    "axes[1, 0].set_title('Linear Interpolation', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(df_missing['timestamp'], mean_values, linewidth=1)\n",
    "axes[1, 1].scatter(df_missing['timestamp'][missing_indices], mean_values[missing_indices], color='red', s=50, zorder=5, label='Imputed')\n",
    "axes[1, 1].set_title('Mean Imputation', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí≠ Reflection: When to use which method?\n",
    "\n",
    "- **Forward/Backward Fill**: Best when ___________\n",
    "- **Interpolation**: Best when ___________\n",
    "- **Mean Imputation**: Best when ___________\n",
    "\n",
    "**Your choice for this data:** ___________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Apply your chosen method\n",
    "df_clean = df_dirty.copy()\n",
    "df_clean['value'] = df_clean['value'].interpolate(method='linear')  # Change this to your preferred method\n",
    "\n",
    "print(\"‚úÖ Missing values handled!\")\n",
    "print(f\"Remaining missing values: {df_clean['value'].isna().sum()}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Technique 2: Noise Reduction (7 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Moving Average (Simple Smoothing)\n",
    "def moving_average(data, window_size):\n",
    "    return data.rolling(window=window_size, center=True).mean()\n",
    "\n",
    "# Exponential Smoothing\n",
    "def exponential_smoothing(data, alpha):\n",
    "    return data.ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "# Apply different smoothing techniques\n",
    "ma_5 = moving_average(df_clean['value'], 5)\n",
    "ma_15 = moving_average(df_clean['value'], 15)\n",
    "exp_01 = exponential_smoothing(df_clean['value'], 0.1)\n",
    "exp_03 = exponential_smoothing(df_clean['value'], 0.3)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Moving average comparison\n",
    "axes[0].plot(df_clean['timestamp'], df_clean['value'], linewidth=0.5, alpha=0.5, label='Original (noisy)')\n",
    "axes[0].plot(df_clean['timestamp'], ma_5, linewidth=2, label='MA (window=5)')\n",
    "axes[0].plot(df_clean['timestamp'], ma_15, linewidth=2, label='MA (window=15)')\n",
    "axes[0].plot(df_clean['timestamp'], df_clean['clean_value'], linewidth=2, linestyle='--', label='True Signal', color='black')\n",
    "axes[0].set_title('Moving Average Smoothing', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Exponential smoothing comparison\n",
    "axes[1].plot(df_clean['timestamp'], df_clean['value'], linewidth=0.5, alpha=0.5, label='Original (noisy)')\n",
    "axes[1].plot(df_clean['timestamp'], exp_01, linewidth=2, label='Exp Smoothing (Œ±=0.1)')\n",
    "axes[1].plot(df_clean['timestamp'], exp_03, linewidth=2, label='Exp Smoothing (Œ±=0.3)')\n",
    "axes[1].plot(df_clean['timestamp'], df_clean['clean_value'], linewidth=2, linestyle='--', label='True Signal', color='black')\n",
    "axes[1].set_title('Exponential Smoothing', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Experiment: Try Different Window Sizes\n",
    "\n",
    "Modify the code below to experiment with different parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# YOUR TURN: Experiment with different parameters\n",
    "window_size = 10  # Try changing this: 3, 5, 10, 20, 30\n",
    "alpha = 0.2  # Try changing this: 0.1, 0.2, 0.5, 0.8\n",
    "\n",
    "# Apply smoothing\n",
    "custom_ma = moving_average(df_clean['value'], window_size)\n",
    "custom_exp = exponential_smoothing(df_clean['value'], alpha)\n",
    "\n",
    "# Visualize your results\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(df_clean['timestamp'], df_clean['value'], linewidth=0.5, alpha=0.4, label='Original')\n",
    "plt.plot(df_clean['timestamp'], custom_ma, linewidth=2, label=f'Your MA (window={window_size})')\n",
    "plt.plot(df_clean['timestamp'], custom_exp, linewidth=2, label=f'Your Exp Smoothing (Œ±={alpha})')\n",
    "plt.title('Your Smoothing Experiment', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Observations:\")\n",
    "print(\"- Larger window size = ___________\")\n",
    "print(\"- Smaller alpha = ___________\")\n",
    "print(\"- Best parameter for this data = ___________\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Apply your chosen smoothing method\n",
    "df_clean['value_smoothed'] = moving_average(df_clean['value'], 10)  # Adjust as needed\n",
    "print(\"‚úÖ Noise reduction applied!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Technique 3: Normalization (6 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create multiple series with different scales\n",
    "series_a = df_clean['value_smoothed'].fillna(method='bfill').fillna(method='ffill')  # Scale: ~50-100\n",
    "series_b = series_a * 10  # Scale: ~500-1000\n",
    "series_c = series_a / 2  # Scale: ~25-50\n",
    "\n",
    "# Normalize using different methods\n",
    "def min_max_scaling(data):\n",
    "    return (data - data.min()) / (data.max() - data.min())\n",
    "\n",
    "def z_score_normalization(data):\n",
    "    return (data - data.mean()) / data.std()\n",
    "\n",
    "# Apply normalizations\n",
    "a_minmax = min_max_scaling(series_a)\n",
    "b_minmax = min_max_scaling(series_b)\n",
    "c_minmax = min_max_scaling(series_c)\n",
    "\n",
    "a_zscore = z_score_normalization(series_a)\n",
    "b_zscore = z_score_normalization(series_b)\n",
    "c_zscore = z_score_normalization(series_c)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "# Original (different scales)\n",
    "axes[0].plot(df_clean['timestamp'], series_a, label='Series A (~50-100)', linewidth=2)\n",
    "axes[0].plot(df_clean['timestamp'], series_b, label='Series B (~500-1000)', linewidth=2)\n",
    "axes[0].plot(df_clean['timestamp'], series_c, label='Series C (~25-50)', linewidth=2)\n",
    "axes[0].set_title('Original Series (Different Scales)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Min-Max Normalized (0-1 range)\n",
    "axes[1].plot(df_clean['timestamp'], a_minmax, label='Series A (normalized)', linewidth=2)\n",
    "axes[1].plot(df_clean['timestamp'], b_minmax, label='Series B (normalized)', linewidth=2)\n",
    "axes[1].plot(df_clean['timestamp'], c_minmax, label='Series C (normalized)', linewidth=2)\n",
    "axes[1].set_title('Min-Max Scaling (0-1 range)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Normalized Value')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Z-score Normalized (mean=0, std=1)\n",
    "axes[2].plot(df_clean['timestamp'], a_zscore, label='Series A (normalized)', linewidth=2)\n",
    "axes[2].plot(df_clean['timestamp'], b_zscore, label='Series B (normalized)', linewidth=2)\n",
    "axes[2].plot(df_clean['timestamp'], c_zscore, label='Series C (normalized)', linewidth=2)\n",
    "axes[2].set_title('Z-Score Normalization (mean=0, std=1)', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Time')\n",
    "axes[2].set_ylabel('Normalized Value')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìè Normalization Statistics:\")\n",
    "print(\"\\nMin-Max Scaling:\")\n",
    "print(f\"  Range: [{a_minmax.min():.2f}, {a_minmax.max():.2f}]\")\n",
    "print(\"\\nZ-Score Normalization:\")\n",
    "print(f\"  Mean: {a_zscore.mean():.2f}\")\n",
    "print(f\"  Std: {a_zscore.std():.2f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí≠ Reflection: Why normalize?\n",
    "\n",
    "**When is normalization needed?**\n",
    "- Comparing multiple time series with different scales\n",
    "- Machine learning models sensitive to scale\n",
    "- Computing distances or similarities\n",
    "\n",
    "**Which method to use?**\n",
    "- Min-Max: ___________\n",
    "- Z-Score: ___________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Technique 4: Outlier Detection (6 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Detect outliers using statistical methods\n",
    "data = df_clean['value'].fillna(method='bfill').fillna(method='ffill')\n",
    "\n",
    "# Method 1: Z-score method (threshold = 3)\n",
    "z_scores = np.abs(stats.zscore(data))\n",
    "outliers_zscore = z_scores > 3\n",
    "\n",
    "# Method 2: IQR method\n",
    "Q1 = data.quantile(0.25)\n",
    "Q3 = data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "outliers_iqr = (data < lower_bound) | (data > upper_bound)\n",
    "\n",
    "# Visualize outlier detection\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Z-score method visualization\n",
    "axes[0, 0].plot(df_clean['timestamp'], data, linewidth=1, label='Data')\n",
    "axes[0, 0].scatter(df_clean['timestamp'][outliers_zscore], data[outliers_zscore], \n",
    "                   color='red', s=100, zorder=5, label=f'Outliers (n={outliers_zscore.sum()})')\n",
    "axes[0, 0].set_title('Z-Score Method (threshold=3)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Value')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# IQR method visualization\n",
    "axes[0, 1].plot(df_clean['timestamp'], data, linewidth=1, label='Data')\n",
    "axes[0, 1].scatter(df_clean['timestamp'][outliers_iqr], data[outliers_iqr], \n",
    "                   color='red', s=100, zorder=5, label=f'Outliers (n={outliers_iqr.sum()})')\n",
    "axes[0, 1].axhline(y=lower_bound, color='orange', linestyle='--', label='IQR bounds')\n",
    "axes[0, 1].axhline(y=upper_bound, color='orange', linestyle='--')\n",
    "axes[0, 1].set_title('IQR Method', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Value')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Boxplot for IQR visualization\n",
    "axes[1, 0].boxplot(data, vert=True)\n",
    "axes[1, 0].scatter([1]*outliers_iqr.sum(), data[outliers_iqr], color='red', s=100, zorder=5, label='Outliers')\n",
    "axes[1, 0].set_title('Boxplot (IQR Method)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Value')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution with outliers marked\n",
    "axes[1, 1].hist(data, bins=30, alpha=0.7, label='Data distribution')\n",
    "axes[1, 1].hist(data[outliers_iqr], bins=30, alpha=0.7, color='red', label='Outliers')\n",
    "axes[1, 1].axvline(x=lower_bound, color='orange', linestyle='--', linewidth=2, label='IQR bounds')\n",
    "axes[1, 1].axvline(x=upper_bound, color='orange', linestyle='--', linewidth=2)\n",
    "axes[1, 1].set_title('Distribution with Outliers', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Value')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Outlier Detection Summary:\")\n",
    "print(f\"Z-score method detected: {outliers_zscore.sum()} outliers\")\n",
    "print(f\"IQR method detected: {outliers_iqr.sum()} outliers\")\n",
    "print(f\"\\nIQR bounds: [{lower_bound:.2f}, {upper_bound:.2f}]\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Different strategies for handling outliers\n",
    "data_cleaned = data.copy()\n",
    "\n",
    "# Strategy 1: Remove outliers (set to NaN)\n",
    "data_removed = data.copy()\n",
    "data_removed[outliers_iqr] = np.nan\n",
    "\n",
    "# Strategy 2: Cap outliers (winsorization)\n",
    "data_capped = data.copy()\n",
    "data_capped[data_capped < lower_bound] = lower_bound\n",
    "data_capped[data_capped > upper_bound] = upper_bound\n",
    "\n",
    "# Strategy 3: Replace with median\n",
    "data_median = data.copy()\n",
    "data_median[outliers_iqr] = data.median()\n",
    "\n",
    "# Visualize strategies\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "axes[0, 0].plot(df_clean['timestamp'], data, linewidth=1)\n",
    "axes[0, 0].scatter(df_clean['timestamp'][outliers_iqr], data[outliers_iqr], color='red', s=50, zorder=5)\n",
    "axes[0, 0].set_title('Original (with outliers)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(df_clean['timestamp'], data_removed, linewidth=1)\n",
    "axes[0, 1].set_title('Strategy 1: Removed (NaN)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(df_clean['timestamp'], data_capped, linewidth=1)\n",
    "axes[1, 0].scatter(df_clean['timestamp'][outliers_iqr], data_capped[outliers_iqr], color='orange', s=50, zorder=5)\n",
    "axes[1, 0].set_title('Strategy 2: Capped at bounds', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Time')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(df_clean['timestamp'], data_median, linewidth=1)\n",
    "axes[1, 1].scatter(df_clean['timestamp'][outliers_iqr], data_median[outliers_iqr], color='green', s=50, zorder=5)\n",
    "axes[1, 1].set_title('Strategy 3: Replaced with median', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Time')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Which strategy to use?\")\n",
    "print(\"- Remove: When outliers are measurement errors\")\n",
    "print(\"- Cap: When you want to preserve the pattern but limit extreme values\")\n",
    "print(\"- Replace: When you want to maintain data continuity\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üë• Peer Review (5 minutes)\n",
    "\n",
    "Exchange notebooks with your neighbor and discuss:\n",
    "\n",
    "1. Which outlier detection method found more outliers?\n",
    "2. Which handling strategy seems most appropriate for this data?\n",
    "3. What are the trade-offs of each approach?\n",
    "\n",
    "**Your conclusions:**\n",
    "\n",
    "___________________________________________\n",
    "\n",
    "___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéì Final Exercise: Complete Pipeline\n",
    "\n",
    "Now apply all techniques to create a complete preprocessing pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Complete preprocessing pipeline\n",
    "def preprocess_time_series(data):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for time series data\n",
    "    \"\"\"\n",
    "    # Step 1: Handle missing values\n",
    "    data_clean = data.interpolate(method='linear')\n",
    "    \n",
    "    # Step 2: Detect outliers\n",
    "    Q1 = data_clean.quantile(0.25)\n",
    "    Q3 = data_clean.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = (data_clean < lower_bound) | (data_clean > upper_bound)\n",
    "    \n",
    "    # Step 3: Handle outliers (cap them)\n",
    "    data_clean[data_clean < lower_bound] = lower_bound\n",
    "    data_clean[data_clean > upper_bound] = upper_bound\n",
    "    \n",
    "    # Step 4: Smooth noise\n",
    "    data_clean = data_clean.rolling(window=10, center=True).mean()\n",
    "    data_clean = data_clean.fillna(method='bfill').fillna(method='ffill')\n",
    "    \n",
    "    # Step 5: Normalize\n",
    "    data_normalized = (data_clean - data_clean.min()) / (data_clean.max() - data_clean.min())\n",
    "    \n",
    "    return data_normalized, outliers\n",
    "\n",
    "# Apply pipeline to our dirty data\n",
    "final_data, detected_outliers = preprocess_time_series(df_dirty['value'])\n",
    "\n",
    "# Visualize before and after\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 8))\n",
    "\n",
    "axes[0].plot(df_dirty['timestamp'], df_dirty['value'], linewidth=1, alpha=0.7)\n",
    "axes[0].set_title('Before Preprocessing', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(df_dirty['timestamp'], final_data, linewidth=2, color='green')\n",
    "axes[1].set_title('After Complete Preprocessing Pipeline', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].set_ylabel('Normalized Value')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Preprocessing Pipeline Complete!\")\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"  - Missing values handled: {df_dirty['value'].isna().sum()}\")\n",
    "print(f\"  - Outliers detected and handled: {detected_outliers.sum()}\")\n",
    "print(f\"  - Data smoothed and normalized\")\n",
    "print(f\"  - Ready for analysis! üéâ\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìù Exit Ticket\n",
    "\n",
    "Before you leave, please answer these questions:\n",
    "\n",
    "### 1. Name ONE difference between time series and sequential data:\n",
    "\n",
    "___________________________________________\n",
    "\n",
    "### 2. Which transformation technique would you use for sensor data with a lot of noise?\n",
    "\n",
    "___________________________________________\n",
    "\n",
    "### 3. One question you still have:\n",
    "\n",
    "___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "1. **Time Series vs Sequential**: Time series has regular time intervals and time is intrinsically important\n",
    "\n",
    "2. **Time Series Components**:\n",
    "   - Trend: Long-term direction\n",
    "   - Seasonality: Regular patterns\n",
    "   - Noise: Random fluctuations\n",
    "\n",
    "3. **Preprocessing Techniques**:\n",
    "   - **Missing data**: Interpolation, forward/backward fill\n",
    "   - **Noise reduction**: Moving average, exponential smoothing\n",
    "   - **Normalization**: Min-max scaling, z-score\n",
    "   - **Outliers**: Z-score method, IQR method\n",
    "\n",
    "4. **Stationarity** is important for many forecasting models\n",
    "\n",
    "---\n",
    "\n",
    "## üîÆ Next Lesson Preview\n",
    "\n",
    "Now that we can prepare time series data, we'll learn how to make **forecasts** using ARIMA models!\n",
    "\n",
    "**See you next time! üëã**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
